# üöÄ Wow Factor Improvements - Top 3 Priorities

This document outlines the three highest-impact improvements to maximize the "wow factor" for the hackathon demo. Total estimated time: **~5.5 hours**.

---

## 1Ô∏è‚É£ Streaming Responses + Agent Thinking Visualization

**Estimated Time:** 2 hours  
**Impact:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê  
**Why It Matters:**
- Shows real-time AI processing instead of a loading spinner
- Demonstrates the LangGraph workflow in action
- Makes waiting feel interactive and transparent
- Judges can see the agent "thinking" through each step

### What We'll Build

#### Backend Changes
- Add Server-Sent Events (SSE) endpoint for streaming
- Modify LangGraph nodes to emit progress updates
- Stream status messages at each workflow step

#### Frontend Changes
- Add EventSource to consume SSE stream
- Display real-time progress indicators
- Show workflow steps as they complete:
  - ‚úì Loaded MultiQC data
  - ‚úì Built vector index
  - ‚Üí Analyzing question...
  - ‚Üí Generating response...

### Implementation Details

**Backend (`api/agent/views.py`):**
```python
from django.http import StreamingHttpResponse
import json

def post(self, request, pk):
    # ... validation code ...
    
    def event_stream():
        # Stream progress updates
        yield f"data: {json.dumps({'type': 'status', 'step': 'load', 'message': 'Loading MultiQC data...'})}\n\n"
        
        yield f"data: {json.dumps({'type': 'status', 'step': 'index', 'message': 'Building vector index...'})}\n\n"
        
        yield f"data: {json.dumps({'type': 'status', 'step': 'analyze', 'message': 'Analyzing question...'})}\n\n"
        
        # Execute agent workflow
        result = agent.chat(run.run_id, question, ...)
        
        yield f"data: {json.dumps({'type': 'status', 'step': 'synthesize', 'message': 'Generating response...'})}\n\n"
        
        # Stream final answer
        yield f"data: {json.dumps({'type': 'answer', 'content': result})}\n\n"
        
        yield "data: [DONE]\n\n"
    
    return StreamingHttpResponse(
        event_stream(),
        content_type='text/event-stream',
        headers={'Cache-Control': 'no-cache', 'X-Accel-Buffering': 'no'}
    )
```

**Frontend (`components/run-chat/useRunChatPanel.ts`):**
```typescript
const [agentStatus, setAgentStatus] = useState<string | null>(null)
const [completedSteps, setCompletedSteps] = useState<string[]>([])

const handleSubmitWithStreaming = (question: string) => {
  const eventSource = new EventSource(
    `/api/runs/${runId}/chat/stream/?question=${encodeURIComponent(question)}`
  )
  
  eventSource.onmessage = (event) => {
    const data = JSON.parse(event.data)
    
    if (data.type === 'status') {
      setAgentStatus(data.message)
      setCompletedSteps(prev => [...prev, data.step])
    } else if (data.type === 'answer') {
      // Update message with final answer
      updateMessage(assistantMessageId, {
        status: 'complete',
        content: data.content,
        // ... other fields
      })
      eventSource.close()
    }
  }
}
```

**Frontend UI (`components/run-chat/ChatMessage.tsx`):**
```tsx
{message.status === 'pending' && (
  <div className="space-y-2">
    <div className="flex items-center gap-2">
      <Spinner className="h-4 w-4" />
      <span className="text-sm font-medium">Agent is thinking...</span>
    </div>
    <div className="space-y-1 text-xs text-muted-foreground">
      <div className="flex items-center gap-2">
        <CheckCircle className="h-3 w-3 text-green-600" />
        <span>Loaded MultiQC data</span>
      </div>
      <div className="flex items-center gap-2">
        <CheckCircle className="h-3 w-3 text-green-600" />
        <span>Built vector index</span>
      </div>
      <div className="flex items-center gap-2">
        <Loader2 className="h-3 w-3 animate-spin text-blue-600" />
        <span className="text-blue-600">Analyzing question...</span>
      </div>
    </div>
  </div>
)}
```

### Files to Modify
- `backend/singlecell_ai_insights/api/agent/views.py`
- `backend/singlecell_ai_insights/services/agent/graph.py`
- `frontend/src/components/run-chat/useRunChatPanel.ts`
- `frontend/src/components/run-chat/ChatMessage.tsx`
- `frontend/src/components/run-chat/types.ts`

---

## 2Ô∏è‚É£ Suggested Questions + Quick Metrics Dashboard

**Estimated Time:** 1.5 hours  
**Impact:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê  
**Why It Matters:**
- Removes friction - judges know exactly what to ask
- Immediate visual impact with metrics dashboard
- Shows data at a glance without needing to chat
- Demonstrates the breadth of agent capabilities

### What We'll Build

#### Suggested Questions Component
- Display 5-6 example questions as clickable buttons
- Questions showcase different agent capabilities:
  - Comparison: "Which samples have high duplication rates?"
  - Analysis: "Are there any outliers in this run?"
  - Metrics: "What's the average GC content?"
  - Visualization: "Show me quality score distribution"
  - Troubleshooting: "Which samples failed QC tests?"

#### Quick Metrics Dashboard
- Replace placeholder metrics card with real data
- Show 3-4 key metrics with progress bars
- Color-coded indicators (green/yellow/red)
- Pulled from MultiQC data when available

### Implementation Details

**Frontend (`components/run-chat/SuggestedQuestions.tsx`):**
```tsx
const SUGGESTED_QUESTIONS = [
  {
    text: "Which samples have high duplication rates?",
    icon: "üìä",
    category: "comparison"
  },
  {
    text: "Are there any outliers in this run?",
    icon: "üîç",
    category: "analysis"
  },
  {
    text: "What's the average GC content?",
    icon: "üìà",
    category: "metrics"
  },
  {
    text: "Show me quality score distribution",
    icon: "üìâ",
    category: "visualization"
  },
  {
    text: "Which samples failed QC tests?",
    icon: "‚ö†Ô∏è",
    category: "troubleshooting"
  },
]

export function SuggestedQuestions({ onSelect }: { onSelect: (q: string) => void }) {
  return (
    <div className="space-y-3">
      <p className="text-sm font-medium text-muted-foreground">
        Try asking the agent:
      </p>
      <div className="grid gap-2 sm:grid-cols-2">
        {SUGGESTED_QUESTIONS.map((q) => (
          <Button
            key={q.text}
            variant="outline"
            size="sm"
            className="justify-start text-left h-auto py-2"
            onClick={() => onSelect(q.text)}
          >
            <span className="mr-2">{q.icon}</span>
            <span className="text-xs">{q.text}</span>
          </Button>
        ))}
      </div>
    </div>
  )
}
```

**Frontend (`components/QuickMetricsDashboard.tsx`):**
```tsx
import { Progress } from "@/components/ui/progress"
import { Card, CardContent, CardHeader, CardTitle } from "@/components/ui/card"

type Metric = {
  label: string
  value: number
  max: number
  unit: string
  status: 'good' | 'warning' | 'error'
}

export function QuickMetricsDashboard({ runId }: { runId: number }) {
  const { data: metrics, isLoading } = useRunMetricsQuery(runId)
  
  if (isLoading) return <Spinner />
  
  const displayMetrics: Metric[] = [
    {
      label: "Avg Quality Score",
      value: metrics?.avgQuality ?? 0,
      max: 40,
      unit: "",
      status: metrics?.avgQuality > 30 ? 'good' : 'warning'
    },
    {
      label: "Duplication Rate",
      value: metrics?.avgDuplication ?? 0,
      max: 100,
      unit: "%",
      status: metrics?.avgDuplication < 20 ? 'good' : 'warning'
    },
    {
      label: "Samples Passed",
      value: metrics?.passedSamples ?? 0,
      max: metrics?.totalSamples ?? 1,
      unit: "",
      status: (metrics?.passedSamples / metrics?.totalSamples) > 0.8 ? 'good' : 'error'
    },
  ]
  
  return (
    <Card>
      <CardHeader>
        <CardTitle>Quick Metrics</CardTitle>
      </CardHeader>
      <CardContent className="space-y-4">
        {displayMetrics.map((metric) => (
          <div key={metric.label}>
            <div className="flex justify-between text-sm mb-1">
              <span className="text-muted-foreground">{metric.label}</span>
              <span className={cn(
                "font-medium",
                metric.status === 'good' && "text-green-600",
                metric.status === 'warning' && "text-yellow-600",
                metric.status === 'error' && "text-red-600"
              )}>
                {metric.value}{metric.unit}
                {metric.max > 1 && ` / ${metric.max}`}
              </span>
            </div>
            <Progress
              value={(metric.value / metric.max) * 100}
              className={cn(
                metric.status === 'good' && "bg-green-100",
                metric.status === 'warning' && "bg-yellow-100",
                metric.status === 'error' && "bg-red-100"
              )}
            />
          </div>
        ))}
      </CardContent>
    </Card>
  )
}
```

**Backend (add endpoint for quick metrics):**
```python
# In api/runs/views.py
class RunViewSet(viewsets.ReadOnlyModelViewSet):
    # ... existing code ...
    
    @action(detail=True, methods=['get'])
    def quick_metrics(self, request, pk=None):
        """Return quick overview metrics for dashboard."""
        run = self.get_object()
        
        # Load MultiQC data and extract key metrics
        try:
            multiqc_data = load_multiqc_data(run.run_id)
            general_stats = multiqc_data.get('report_general_stats_data', [])
            
            # Calculate averages
            avg_quality = calculate_avg(general_stats, 'mean_quality')
            avg_duplication = calculate_avg(general_stats, 'percent_duplicates')
            passed_samples = sum(1 for s in general_stats if s.get('status') == 'PASS')
            
            return Response({
                'avgQuality': avg_quality,
                'avgDuplication': avg_duplication,
                'passedSamples': passed_samples,
                'totalSamples': len(general_stats),
            })
        except Exception as e:
            return Response({'error': str(e)}, status=500)
```

### Files to Create/Modify
- `frontend/src/components/run-chat/SuggestedQuestions.tsx` (new)
- `frontend/src/components/QuickMetricsDashboard.tsx` (new)
- `frontend/src/pages/RunDetailPage.tsx` (integrate both components)
- `backend/singlecell_ai_insights/api/runs/views.py` (add quick_metrics endpoint)
- `frontend/src/api/runs.ts` (add useRunMetricsQuery hook)

---

## 3Ô∏è‚É£ Agent Confidence Score + Explanations

**Estimated Time:** 2 hours  
**Impact:** ‚≠ê‚≠ê‚≠ê‚≠ê  
**Why It Matters:**
- Shows transparency in AI reasoning
- Builds trust with judges
- Demonstrates sophisticated agent capabilities
- Differentiates from simple chatbots

### What We'll Build

#### Confidence Calculation
- Calculate confidence based on:
  - Number of relevant documents retrieved
  - Semantic similarity scores
  - Presence of metric data
  - Quality of vector search results

#### Confidence Display
- Show confidence percentage (0-100%)
- Display visual progress bar
- Provide explanation of confidence level
- Color-coded indicators

### Implementation Details

**Backend (`services/agent/nodes/synthesis.py`):**
```python
def calculate_confidence(state):
    """Calculate confidence score for the answer."""
    confidence = 0
    reasons = []
    
    # Factor 1: Number of retrieved documents (max 40 points)
    num_docs = len(state.get('retrieved', []))
    if num_docs >= 5:
        confidence += 40
        reasons.append(f"Found {num_docs} relevant documents")
    elif num_docs >= 3:
        confidence += 25
        reasons.append(f"Found {num_docs} documents")
    else:
        confidence += 10
        reasons.append(f"Limited documents found ({num_docs})")
    
    # Factor 2: Semantic similarity (max 30 points)
    avg_similarity = state.get('avg_similarity', 0)
    if avg_similarity > 0.8:
        confidence += 30
        reasons.append("High semantic similarity (>0.8)")
    elif avg_similarity > 0.6:
        confidence += 20
        reasons.append("Good semantic similarity")
    else:
        confidence += 10
        reasons.append("Moderate similarity")
    
    # Factor 3: Metric data availability (max 20 points)
    if state.get('metric_key'):
        confidence += 20
        reasons.append("Specific metric data available")
    elif state.get('samples'):
        confidence += 10
        reasons.append("Sample data available")
    
    # Factor 4: Question clarity (max 10 points)
    question_length = len(state.get('question', '').split())
    if 5 <= question_length <= 20:
        confidence += 10
        reasons.append("Clear, well-formed question")
    else:
        confidence += 5
    
    return min(confidence, 100), reasons


def synthesize(state):
    """Generate final answer with confidence score."""
    # ... existing synthesis code ...
    
    confidence, reasons = calculate_confidence(state)
    
    state['confidence'] = confidence
    state['confidence_explanation'] = " ‚Ä¢ ".join(reasons)
    
    # Add confidence level to answer
    if confidence < 50:
        state['notes'] = state.get('notes', []) + [
            "‚ö†Ô∏è Low confidence - answer may be incomplete or uncertain"
        ]
    
    return state
```

**Backend (`api/agent/serializers.py`):**
```python
class MessageSerializer(serializers.ModelSerializer):
    # ... existing fields ...
    confidence = serializers.IntegerField(required=False)
    confidence_explanation = serializers.CharField(required=False)
    
    class Meta:
        model = Message
        fields = [
            'id', 'role', 'content', 'citations', 'notes',
            'metric_key', 'created_at', 'confidence', 'confidence_explanation'
        ]
```

**Backend (`models/conversation.py`):**
```python
class Message(models.Model):
    # ... existing fields ...
    confidence = models.IntegerField(null=True, blank=True)
    confidence_explanation = models.TextField(blank=True)
```

**Frontend (`components/run-chat/ChatMessage.tsx`):**
```tsx
import { Progress } from "@/components/ui/progress"
import { Info } from "lucide-react"
import { Tooltip, TooltipContent, TooltipTrigger } from "@/components/ui/tooltip"

export function ChatMessage({ message }: { message: ChatMessage }) {
  const getConfidenceColor = (confidence: number) => {
    if (confidence >= 80) return "text-green-600"
    if (confidence >= 60) return "text-yellow-600"
    return "text-red-600"
  }
  
  const getConfidenceLabel = (confidence: number) => {
    if (confidence >= 80) return "High confidence"
    if (confidence >= 60) return "Moderate confidence"
    return "Low confidence"
  }
  
  return (
    <div className="space-y-2">
      {/* Message content */}
      <div className="prose prose-sm">
        {message.content}
      </div>
      
      {/* Confidence indicator */}
      {message.role === 'assistant' && message.confidence != null && (
        <div className="flex items-center gap-3 pt-2 border-t">
          <div className="flex items-center gap-2 text-xs">
            <span className="text-muted-foreground">Confidence:</span>
            <Progress value={message.confidence} className="w-20 h-2" />
            <span className={cn("font-medium", getConfidenceColor(message.confidence))}>
              {message.confidence}%
            </span>
          </div>
          
          {message.confidence_explanation && (
            <Tooltip>
              <TooltipTrigger>
                <Info className="h-3 w-3 text-muted-foreground" />
              </TooltipTrigger>
              <TooltipContent className="max-w-xs">
                <p className="text-xs">{message.confidence_explanation}</p>
              </TooltipContent>
            </Tooltip>
          )}
          
          <span className={cn(
            "text-xs font-medium ml-auto",
            getConfidenceColor(message.confidence)
          )}>
            {getConfidenceLabel(message.confidence)}
          </span>
        </div>
      )}
    </div>
  )
}
```

### Files to Modify
- `backend/singlecell_ai_insights/services/agent/nodes/synthesis.py`
- `backend/singlecell_ai_insights/models/conversation.py` (add migration)
- `backend/singlecell_ai_insights/api/agent/serializers.py`
- `frontend/src/components/run-chat/ChatMessage.tsx`
- `frontend/src/components/run-chat/types.ts`
- `frontend/src/api/runs.ts`

---

## üéØ Implementation Order

1. **Start with #2 (Suggested Questions + Metrics)** - Quickest win, immediate visual impact
2. **Then #3 (Confidence Score)** - Adds sophistication, good foundation
3. **Finish with #1 (Streaming)** - Most complex, but builds on the others

## üìä Expected Impact

After implementing all three:
- **Demo flow**: Judges see metrics ‚Üí click suggested question ‚Üí watch agent think ‚Üí see confident answer
- **Wow moments**: 
  - Real-time streaming shows "AI at work"
  - Confidence scores show "AI transparency"
  - Suggested questions show "AI breadth"
- **Winning potential**: Jumps from **7/10 to 9/10** ‚ú®

---

## ‚úÖ Checklist

- [ ] Implement Suggested Questions component
- [ ] Implement Quick Metrics Dashboard
- [ ] Add backend endpoint for quick metrics
- [ ] Implement confidence calculation in synthesis node
- [ ] Add confidence fields to Message model + migration
- [ ] Update serializers for confidence
- [ ] Add confidence UI to ChatMessage component
- [ ] Implement SSE streaming endpoint
- [ ] Add EventSource handling in frontend
- [ ] Create agent thinking visualization
- [ ] Test all three features together
- [ ] Record demo showcasing all improvements

**Let's build this! üöÄ**
